---
title: "p8105_hw5_dl3757"
author: "Dang Lin dl3757"
date: "2024-11-15"
output: github_document
---

```{r message = FALSE, warning = FALSE}
library(tidyverse)

set.seed(3757)
```

# Problem 1
```{r message = FALSE, warning = FALSE}
# Write a function to check if there are duplicate birthdays in a group
birthday_sim = function(n) {

  bdays = sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n

  return(duplicate)
  
}

# Check whether there are duplicate birthdays in a group of 10 people
knitr::kable(birthday_sim(10), col.name = "Return Result", 
             caption = "Group Size of 10")
```

```{r}
# Run the function 10000 times for each group size between 2 and 50
sim_res = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) %>% 
  mutate(res = map_lgl(n, birthday_sim)) %>%
  group_by(n) %>% 
  summarize(prob = mean(res))

# Make a plot showing the probability as a function of group size
sim_res %>%
  ggplot(aes(x = n, y = prob )) + 
  geom_line() + 
  labs(
    title = "Probability of Shared Birthday vs. Group Size", 
    x = "Group Size",
    y = "Probability") +
  theme_minimal()
```

From the plot, it can be observed that the probability of at least two people sharing a birthday increases as the group size grows. When the group size is small, the probability increases gradually. However, as the group size becomes larger, the probability rises more rapidly. Eventually, the probability approaches 1 as the group size becomes sufficiently large.

# Problem 2
## (a)
```{r message = FALSE, warning = FALSE}
# Set parameters
n <- 30
sigma <- 5 
mu <- 0
alpha <- 0.05
iterations <- 5000

# Write a function that return t test results
simulate_t_test <- function(n, mu, sigma) {
  simulate_data <- rnorm(n, mean = mu, sd = sigma)
  
  t_test_results <- t.test(simulate_data, mu = 0) %>% 
    broom::tidy() %>% 
    select(estimate = estimate, 
           p.value = p.value)
  
  return(t_test_results)
}

# Generate 5000 datasets from the model
simulate_results_df_1 <- 
  expand_grid(
  true_mu = mu,
  iter = 1:iterations) %>%
  mutate(
    result = map(true_mu, ~ simulate_t_test(n, .x, sigma))
  ) %>%
  unnest(result)

# Create a table with head 10 simulations
knitr::kable(head(simulate_results_df_1, 10), 
             col.names = c("True Mu", "Iterations", 
                           "Estimate", "P-value"))
```

## (b)
```{r message = FALSE, warning = FALSE}
# Run simulations for different mu values
simulate_results_df_2 <- 
  expand_grid(
  true_mu = c(1, 2, 3, 4, 5, 6),
  iter = 1:iterations) %>%
  mutate(
    result = map(true_mu, ~ simulate_t_test(n, .x, sigma))
  ) %>%
  unnest(result)

# Calculate power for each mu value
power_results <- simulate_results_df_2 %>%
  group_by(true_mu) %>%
  summarize(power = mean(p.value < alpha))

# Make a plot showing the proportion of times the null was rejected
ggplot(power_results, aes(x = true_mu, y = power)) +
  geom_line() +
  geom_point() + 
  labs(title = "Power of One-Sample t-Test vs. True Mu",
       x = expression("True Value of " * mu),
       y = "Power") +
  theme_minimal()
```

The power increases as the effect size grows, indicating that a larger effect size makes it easier for the t-test to distinguish the true mean from the null hypothesis.